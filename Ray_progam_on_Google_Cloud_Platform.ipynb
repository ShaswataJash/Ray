{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyNPDdDeZf18oKForOnqksts",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaswataJash/Ray/blob/master/Ray_progam_on_Google_Cloud_Platform.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPwmlnDBW9Kv"
      },
      "source": [
        "**INTRODUCTION: The notebook demonstrates how to create ray-cluster (https://docs.ray.io/en/latest/installation.html) on Google Cloud Platform (GCP) and then run a ray-program remotely on it from google-colab notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_6yMZpERnaK"
      },
      "source": [
        "!whoami\n",
        "!pwd\n",
        "!python -V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ricEd2ooWfZ3"
      },
      "source": [
        "**Install ray library - current notebook is tested against 2.2.0 version of ray**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu3x5TY_bDs6"
      },
      "source": [
        "!pip install ray==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWE2zI_cK0qC"
      },
      "source": [
        "**Run following cell only during development in local node. If you run following cell, it will create a local ray-cluster with single node. The single node acts as head-node of the cluster. Develop your ray-program using this local cluster. Once you have successfully developed the ray-program, then save that ray-program as python (.py) file which can be later submitted to a remote ray-cluster running in Google Cloud Platform (GCP) compute cluster.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQpKfN3KkrGe"
      },
      "source": [
        "!ray stop\n",
        "!rm -Rf /tmp/ray\n",
        "!ray start --head --port=6379 --object-manager-port=8076 --include-dashboard=True --disable-usage-stats --verbose &"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc0Zl0B6MVyR"
      },
      "source": [
        "**Create the following directory in local node which will be used to write final output from ray-program. When you are developing the ray-program locally, write the output to this directory present in local node. We will be mapping the same directoy path even in remote ray-cluster so that there is no need to change the ray-program whether it is running locally or remotely.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8VNaCfPJ6rE"
      },
      "source": [
        "!mkdir /tmp/ray_local_directory_mount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqJmM-8nM8gB"
      },
      "source": [
        "**Note the %%writefile magic code of jupyter notebook used in the following cell. When you are developing the ray-program locally comment out %%writefile as** \n",
        "\n",
        "`#%%writefile ray_test.py`\n",
        "\n",
        "**Once the ray-program is developed successfully in local node, write the content of the following cell as python(.py) file which can be now submitted to remote ray cluster.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Jqj3aSk1x9",
        "cellView": "code"
      },
      "source": [
        "#test program taken from https://towardsdatascience.com/how-to-scale-python-on-every-major-cloud-provider-5e5df3e88274\n",
        "\n",
        "%%writefile ray_test.py\n",
        "\n",
        "from collections import Counter\n",
        "import socket\n",
        "import os\n",
        "import time\n",
        "import ray\n",
        "\n",
        "ray.shutdown()\n",
        "ray.init(address='auto', ignore_reinit_error=True)\n",
        "\n",
        "print('''This cluster consists of {} nodes in total {} CPU resources in total '''.format(len(ray.nodes()), ray.cluster_resources()['CPU']))\n",
        "\n",
        "@ray.remote(num_cpus=0.5)\n",
        "def f():\n",
        "    time.sleep(0.001)\n",
        "    # Return IP address.\n",
        "    return socket.gethostbyname(socket.gethostname()), os.getpid() \n",
        "\n",
        "object_ids = [f.remote() for _ in range(1000000)]\n",
        "ipaddresses_pids = ray.get(object_ids)\n",
        "\n",
        "with open('/tmp/ray_local_directory_mount/task_stat.txt', 'w') as writer:\n",
        "    print('Tasks executed')\n",
        "    for (ip_address, pid), num_tasks in Counter(ipaddresses_pids).items():\n",
        "        rowToWrite = '{} tasks on {}:pid{}\\n'.format(num_tasks, ip_address, pid)\n",
        "        print(rowToWrite)\n",
        "        writer.write(rowToWrite)\n",
        "\n",
        "ray.shutdown()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl4UX4J8ODew"
      },
      "source": [
        "**Run following cell only during development in local node. It will show the content written by ray-program and then close the local ray-cluster (a single node cluster with only head-node).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1fF7kt-lGK8"
      },
      "source": [
        "!cat /tmp/ray_local_directory_mount/task_stat.txt\n",
        "!ray stop -v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyx0ZOc9WF5w"
      },
      "source": [
        "**First install google-api-python-client and cryptography library for ray to setup remote cluster in GCP.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFZ-3GVubTK7"
      },
      "source": [
        "!pip install google-api-python-client==2.75.0\n",
        "!pip install cryptography==39.0.0 #needed internally by ray up"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnMxd9ofl4yP"
      },
      "source": [
        "#refer: https://cloud.google.com/iam/docs/service-accounts (Go to 'User-managed service accounts' section)\n",
        "### create a new user-managed service account for ray-experiment from 'IAM & Admin' > 'Service Account') \n",
        "### Give only 'Editor' role for the GCP project (it is good practise not to give 'owner' access to the project)\n",
        "#Refer: https://cloud.google.com/iam/docs/creating-managing-service-account-keys for how to generate the service-account json file\n",
        "#Rename that file as 'gcp_service_account.json' and then upload to google colab using 'upload to session storage' icon\n",
        "import os\n",
        "import json\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/gcp_service_account.json\"\n",
        "\n",
        "with open(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"], 'r') as gcp_service_acc_file:\n",
        "    data = json.load(gcp_service_acc_file)\n",
        "    os.environ[\"CLOUDSDK_CORE_PROJECT\"] = data['project_id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "refer: https://cloud.google.com/compute/docs/images#gcloud"
      ],
      "metadata": {
        "id": "34h3t8UIKRtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "id": "KYvcO0S5SYpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud compute images list --format=\"csv(diskSizeGb,archiveSizeBytes,NAME,PROJECT,FAMILY)\" > compute_images_list.csv"
      ],
      "metadata": {
        "id": "IFeE6KEbVxaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "compute_images_list_df = pd.read_csv('compute_images_list.csv')\n",
        "compute_images_list_df.sort_values(by=['archive_size_bytes'], inplace=True)\n",
        "pd.set_option('display.max_rows', compute_images_list_df.shape[0]+1)\n",
        "compute_images_list_df"
      ],
      "metadata": {
        "id": "H11DAXHhnzcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!gcloud compute images list --project deeplearning-platform-release --no-standard-images --format=\"table(diskSizeGb,NAME,PROJECT,FAMILY)\" \n",
        "!gcloud compute images list --project deeplearning-platform-release --no-standard-images --format=\"csv(diskSizeGb,archiveSizeBytes,NAME,PROJECT,FAMILY)\" > deeplearning_platform_release_image_list.csv"
      ],
      "metadata": {
        "id": "udWUR12tVtBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "deeplearning_platform_release_image_list_df = pd.read_csv('deeplearning_platform_release_image_list.csv')\n",
        "deeplearning_platform_release_image_list_df.sort_values(by=['archive_size_bytes'], inplace=True)\n",
        "pd.set_option('display.max_rows', deeplearning_platform_release_image_list_df.shape[0]+1)\n",
        "deeplearning_platform_release_image_list_df"
      ],
      "metadata": {
        "id": "Z68A07ctlP7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yauxxMPlZO6t"
      },
      "source": [
        "**Following yaml file is the overall configuration file which will define topology of the remote ray-cluster. Note the {{GCP_PROJECT_ID}}, which will be later replaced by your GCP-project-id extracted from your service-account key file (in JSON form). Note that total number of CPUs that you can allocate will be driven by quota-limit in your GCP account. For an example, a free-trial account may not able to allocate more than 8 CPUs. Below I have considered you have enabled billing in your GCP account. In a billable account, GCP allows default 24 CPUs to be allocated in every region. Considering that, the following configuration will create 1 head node of 4 CPUs and 10 additional worker nodes of 2 CPUs.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vwF-_3rcSY1"
      },
      "source": [
        "#refer: https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/gcp/example-full.yaml\n",
        "\n",
        "%%writefile ray_gcp_cluster.yaml\n",
        "\n",
        "# An unique identifier for the head node and workers of this cluster.\n",
        "cluster_name: \"ray-exp1\"\n",
        "\n",
        "# The minimum number of workers nodes to launch in addition to the head\n",
        "# node. This number should be >= 0.\n",
        "min_workers: 10\n",
        "\n",
        "# The maximum number of workers nodes to launch in addition to the head\n",
        "# node. This takes precedence over min_workers.\n",
        "max_workers: 10\n",
        "\n",
        "# The autoscaler will scale up the cluster faster with higher upscaling speed.\n",
        "# E.g., if the task requires adding more nodes then autoscaler will gradually\n",
        "# scale up the cluster in chunks of upscaling_speed*currently_running_nodes.\n",
        "# This number should be > 0.\n",
        "#upscaling_speed: 1.0\n",
        "\n",
        "# This executes all commands on all nodes in the docker container,\n",
        "# and opens all the necessary ports to support the Ray cluster.\n",
        "# Empty string means disabled.\n",
        "docker:\n",
        "  #image: \"rayproject/ray-ml:latest-cpu\" # You can change this to latest-gpu if you require gpu support\n",
        "  image: rayproject/ray:latest-cpu   # use this one if you don't need ML dependencies, it's faster to pull\n",
        "  container_name: \"ray_container\"\n",
        "  # If true, pulls latest version of image. Otherwise, `docker run` will only pull the image\n",
        "  # if no cached version is present.\n",
        "  pull_before_run: False\n",
        "  run_options:  # Extra options to pass into \"docker run\"\n",
        "    - --ulimit nofile=65536:65536\n",
        "\n",
        "  # Example of running a GPU head with CPU workers\n",
        "  # head_image: \"rayproject/ray-ml:latest-gpu\"\n",
        "  # Allow Ray to automatically detect GPUs\n",
        "\n",
        "  # worker_image: \"rayproject/ray-ml:latest-cpu\"\n",
        "  # worker_run_options: []\n",
        "\n",
        "# If a node is idle for this many minutes, it will be removed.\n",
        "#idle_timeout_minutes: 2\n",
        "\n",
        "# Cloud-provider specific configuration.\n",
        "provider:\n",
        "    type: gcp\n",
        "    region: us-central1\n",
        "    availability_zone: us-central1-a\n",
        "    project_id: {{GCP_PROJECT_ID}} # Globally unique project id <USER HAD TO USE THEIR OWN GCP PROJECT-ID>\n",
        "\n",
        "# How Ray will authenticate with newly launched nodes.\n",
        "auth:\n",
        "    ssh_user: ray_user\n",
        "# By default Ray creates a new private keypair, but you can also use your own.\n",
        "# If you do so, make sure to also set \"KeyName\" in the head and worker node\n",
        "# configurations below. This requires that you have added the key into the\n",
        "# project wide meta-data.\n",
        "#    ssh_private_key: /path/to/your/key.pem\n",
        "\n",
        "# Tell the autoscaler the allowed node types and the resources they provide.\n",
        "# The key is the name of the node type, which is just for debugging purposes.\n",
        "# The node config specifies the launch config and physical instance type.\n",
        "available_node_types:\n",
        "    ray_head_default:\n",
        "        # The resources provided by this node type.\n",
        "        resources: {\"CPU\": 4}\n",
        "        # Provider-specific config for the head node, e.g. instance type. By default\n",
        "        # Ray will auto-configure unspecified fields such as subnets and ssh-keys.\n",
        "        # For more documentation on available fields, see:\n",
        "        # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert\n",
        "        node_config:\n",
        "            machineType: n1-standard-4\n",
        "            disks:\n",
        "              - boot: true\n",
        "                autoDelete: true\n",
        "                type: PERSISTENT\n",
        "                initializeParams:\n",
        "                  diskSizeGb: 10\n",
        "                  # See https://cloud.google.com/compute/docs/images for more images (choose only thos images which have rsync installed)\n",
        "                  #sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu\n",
        "                  sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2210-amd64\n",
        "\n",
        "            # Additional options can be found in in the compute docs at\n",
        "            # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert\n",
        "\n",
        "            # If the network interface is specified as below in both head and worker\n",
        "            # nodes, the manual network config is used.  Otherwise an existing subnet is\n",
        "            # used.  To use a shared subnet, ask the subnet owner to grant permission\n",
        "            # for 'compute.subnetworks.use' to the ray autoscaler account...\n",
        "            # networkInterfaces:\n",
        "            #   - kind: compute#networkInterface\n",
        "            #     subnetwork: path/to/subnet\n",
        "            #     aliasIpRanges: []\n",
        "    ray_worker_small:\n",
        "        # The minimum number of worker nodes of this type to launch.\n",
        "        # This number should be >= 0.\n",
        "        min_workers: 10\n",
        "        # The maximum number of worker nodes of this type to launch.\n",
        "        # This takes precedence over min_workers. (This should match with max_workers field kept on the top.)\n",
        "        max_workers: 10\n",
        "        # The resources provided by this node type.\n",
        "        resources: {\"CPU\": 2}\n",
        "        # Provider-specific config for the head node, e.g. instance type. By default\n",
        "        # Ray will auto-configure unspecified fields such as subnets and ssh-keys.\n",
        "        # For more documentation on available fields, see:\n",
        "        # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert\n",
        "        node_config:\n",
        "            machineType: n1-standard-2\n",
        "            disks:\n",
        "              - boot: true\n",
        "                autoDelete: true\n",
        "                type: PERSISTENT\n",
        "                initializeParams:\n",
        "                  diskSizeGb: 10\n",
        "                  # See https://cloud.google.com/compute/docs/images for more images (choose only thos images which have rsync installed)\n",
        "                  #sourceImage: projects/deeplearning-platform-release/global/images/family/common-cpu\n",
        "                  sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2210-amd64\n",
        "            # Run workers on preemtible instance by default.\n",
        "            # Comment this out to use on-demand.\n",
        "            scheduling:\n",
        "              - preemptible: true\n",
        "            # Un-Comment this to launch workers with the Service Account of the Head Node\n",
        "            # serviceAccounts:\n",
        "            # - email: ray-autoscaler-sa-v1@<project_id>.iam.gserviceaccount.com\n",
        "            #   scopes:\n",
        "            #   - https://www.googleapis.com/auth/cloud-platform\n",
        "\n",
        "    # Additional options can be found in in the compute docs at\n",
        "    # https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert\n",
        "\n",
        "# Specify the node type of the head node (as configured above).\n",
        "head_node_type: ray_head_default\n",
        "\n",
        "# Files or directories to copy to the head and worker nodes. The format is a\n",
        "# dictionary from REMOTE_PATH: LOCAL_PATH, e.g.\n",
        "file_mounts: {\n",
        "#    \"/path1/on/remote/machine\": \"/path1/on/local/machine\",\n",
        "#    \"/path2/on/remote/machine\": \"/path2/on/local/machine\",\n",
        "}\n",
        "\n",
        "# Files or directories to copy from the head node to the worker nodes. The format is a\n",
        "# list of paths. The same path on the head node will be copied to the worker node.\n",
        "# This behavior is a subset of the file_mounts behavior. In the vast majority of cases\n",
        "# you should just use file_mounts. Only use this if you know what you're doing!\n",
        "cluster_synced_files: []\n",
        "\n",
        "# Whether changes to directories in file_mounts or cluster_synced_files in the head node\n",
        "# should sync to the worker node continuously\n",
        "file_mounts_sync_continuously: False\n",
        "\n",
        "# Patterns for files to exclude when running rsync up or rsync down\n",
        "rsync_exclude:\n",
        "    - \"**/.git\"\n",
        "    - \"**/.git/**\"\n",
        "\n",
        "# Pattern files to use for filtering out files when running rsync up or rsync down. The file is searched for\n",
        "# in the source directory and recursively through all subdirectories. For example, if .gitignore is provided\n",
        "# as a value, the behavior will match git's behavior for finding and using .gitignore files.\n",
        "rsync_filter:\n",
        "    - \".gitignore\"\n",
        "\n",
        "# List of commands that will be run before `setup_commands`. If docker is\n",
        "# enabled, these commands will run outside the container and before docker\n",
        "# is setup.\n",
        "initialization_commands:\n",
        "    # Wait for auto upgrade that might run in the background (refer #refer https://itsfoss.com/could-not-get-lock-error/ and https://github.com/ray-project/ray/issues/15893)\n",
        "    - bash -c $'ps -e | grep apt | awk \\'{print $1}\\' | xargs tail -f --pid || true' \n",
        "    #get the OS info\n",
        "    - uname -a\n",
        "    - cat /etc/*-release\n",
        "    - lsb_release -a\n",
        "    #get the logged in user related info\n",
        "    - id\n",
        "    - w\n",
        "    #disable ipv6 (https://itsfoss.com/disable-ipv6-ubuntu-linux/)\n",
        "    - >-\n",
        "      sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1;\n",
        "      sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1;\n",
        "      sudo sysctl -w net.ipv6.conf.lo.disable_ipv6=1\n",
        "    #firewall is not enabled by default in ubuntu (list out iptable rules to verify that)\n",
        "    - sudo iptables -L\n",
        "    #we have to install docker for images coming from 'ubuntu-os-cloud' project (but for 'deeplearning-platform-release' docker is pre-installed in the image) \n",
        "    #But note 'ubuntu-os-cloud' images requires just 10GB of space whereas 'deeplearning-platform-release' requires minimum 30GB of space\n",
        "    - sudo apt-get update\n",
        "    - sudo apt -y install docker.io\n",
        "    - sudo snap install docker\n",
        "    - sudo usermod -a -G docker ray_user\n",
        "    # refer PaulFenton's comments in https://github.com/ray-project/ray/issues/15893\n",
        "    #- sudo rm /var/run/docker.pid\n",
        "    #refer https://stackoverflow.com/questions/43537790/docker-fails-to-start-due-to-volume-store-metadata-database-timeout\n",
        "    - ps axf | grep docker | grep -v grep | awk '{print \"kill -9 \" $1}' | sudo sh\n",
        "    - sudo dockerd --max-download-attempts 10 &\n",
        "    #TODO: check dockerd has started successfully\n",
        "\n",
        "# List of shell commands to run to set up nodes.\n",
        "setup_commands: []    \n",
        "\n",
        "# Custom commands that will be run on the head node after common setup.\n",
        "head_setup_commands:\n",
        "    - mkdir /tmp/ray_local_directory_mount\n",
        "    - python --version\n",
        "    # rayproject docker images have python, google-api-client and cryptography packages installed, so no need to install them in head nodes\n",
        "    #- pip install google-api-python-client==2.75.0\n",
        "    #- pip install --upgrade --force-reinstall cryptography==39.0.0\n",
        "\n",
        "# Custom commands that will be run on worker nodes after common setup.\n",
        "worker_setup_commands: []\n",
        "\n",
        "# Command to start ray on the head node. You don't need to change this. (without webui set to 0.0.0.0, by default it binds to 127.0.0.1)\n",
        "head_start_ray_commands:\n",
        "    - ray stop\n",
        "    - >-\n",
        "      ray start\n",
        "      --head\n",
        "      --port=6379\n",
        "      --object-manager-port=8076\n",
        "      --include-dashboard=True --dashboard-host=0.0.0.0\n",
        "      --disable-usage-stats --verbose\n",
        "      --autoscaling-config=~/ray_bootstrap_config.yaml\n",
        "\n",
        "# Command to start ray on worker nodes. You don't need to change this.\n",
        "worker_start_ray_commands:\n",
        "    - ray stop\n",
        "    - >-\n",
        "      ray start\n",
        "      --address=$RAY_HEAD_IP:6379\n",
        "      --object-manager-port=8076\n",
        "      --disable-usage-stats --verbose"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mLgdhZlC9jZ"
      },
      "source": [
        "import json\n",
        "\n",
        "with open(os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"], 'r') as gcp_service_acc_file:\n",
        "    data = json.load(gcp_service_acc_file)\n",
        "    with open('ray_gcp_cluster.yaml', 'r') as ray_cluster_config_file:\n",
        "        config = ray_cluster_config_file.read().replace('{{GCP_PROJECT_ID}}', data['project_id'])\n",
        "    with open('ray_gcp_cluster.yaml', 'w') as ray_cluster_config_file:\n",
        "        ray_cluster_config_file.write(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJse89i7EmCZ"
      },
      "source": [
        "!cat ray_gcp_cluster.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-nORoprwge"
      },
      "source": [
        "!rm -rf /tmp/*\n",
        "!mkdir -p /tmp/ray_local_directory_mount\n",
        "!echo \"Content of /tmp\"\n",
        "!ls -la /tmp\n",
        "!rm -rf /root/.ssh/*\n",
        "!mkdir -p /root/.ssh\n",
        "!echo \"Content of /root\"\n",
        "!ls -la /root\n",
        "!echo \"Content of /root/.ssh\"\n",
        "!ls -la /root/.ssh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ray disable-usage-stats"
      ],
      "metadata": {
        "id": "8VoR-kKP2F-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "New status: update-failed\n",
        "!!!\n",
        " {'message': 'SSH command failed.'}\n",
        " SSH command failed.\n",
        "!!!\n",
        "```\n",
        "Repeatedly try running 'ray up' command till all required number of worker nodes are ready\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V-coOkH2wPic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ray up --help"
      ],
      "metadata": {
        "id": "fJ1aRlxQzE69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyeGpUmBdg3g"
      },
      "source": [
        "!ray up --yes --verbose ray_gcp_cluster.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ray exec /content/ray_gcp_cluster.yaml 'tail -n 100 -f /tmp/ray/session_latest/logs/monitor*'"
      ],
      "metadata": {
        "id": "3tK-Si6QsCCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ray monitor --lines 5 ray_gcp_cluster.yaml &"
      ],
      "metadata": {
        "id": "H5UpTCC_GCnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ray submit --help"
      ],
      "metadata": {
        "id": "4XEIB44inaSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvukxT5pGAEY"
      },
      "source": [
        "!ray submit ray_gcp_cluster.yaml ray_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w8qSm4GTNl0"
      },
      "source": [
        "!ray rsync-down --verbose ray_gcp_cluster.yaml /tmp/ray_local_directory_mount/task_stat.txt /tmp/ray_local_directory_mount"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUHCnUIrR-cV"
      },
      "source": [
        "!cat /tmp/ray_local_directory_mount/task_stat.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIQwdBUTZWdf"
      },
      "source": [
        "!ray down --yes --verbose ray_gcp_cluster.yaml"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}